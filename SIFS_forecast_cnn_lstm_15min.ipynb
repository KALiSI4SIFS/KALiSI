 {
 "cells": [
  {
   "cell_type": "markdown",
   "id": "379494bb",
   "metadata": {},
   "source": [
    "# CNN-LSTM Model for Solar Irradiance Forecasting (15-min Resolution)\n",
    "\n",
    "This notebook supports the results presented in the related journal article.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade27b9f",
   "metadata": {},
   "source": [
    "## 1. Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "import h5py\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b2954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tensorflow version:\", tf.__version__)\n",
    "gpus =  tf.config.list_physical_devices('GPU')\n",
    "print(\"available gpus:\", gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(\"available cpus:\", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce34836",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = \"F:/SIFS\" \n",
    "\n",
    "data_folder = os.path.join(cwd,\"data_expanded_15min\")\n",
    "data_path = os.path.join(data_folder, \"forecast_dataset.hdf5\")\n",
    "\n",
    "model_name = 'SIFS_forecast_l5min'\n",
    "output_folder = os.path.join(cwd,\"model_output_15min\", model_name)\n",
    "if os.path.isdir(output_folder)==False:\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "\n",
    "print(\"data_folder:\", data_folder)\n",
    "print(\"data_path:\", data_path)\n",
    "print(\"output_folder:\", output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b77bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_dataset = h5py.File(data_path, 'r')\n",
    "\n",
    "def get_all(name):\n",
    "    if name!=None:\n",
    "        print(forecast_dataset[name])\n",
    "    \n",
    "forecast_dataset.visit(get_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f9b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*50)\n",
    "img_side_len = forecast_dataset['trainval']['images_log'].shape[2]\n",
    "num_log_term = forecast_dataset['trainval']['images_log'].shape[1]\n",
    "num_color_channel =forecast_dataset['trainval']['images_log'].shape[4]\n",
    "image_input_dim = [img_side_len,img_side_len,num_log_term*num_color_channel]\n",
    "\n",
    "print(\"image side length:\", img_side_len)\n",
    "print(\"number of log terms:\", num_log_term)\n",
    "print(\"number of color channels:\", num_color_channel)\n",
    "print(\"input image dimension:\", image_input_dim)\n",
    "\n",
    "times_trainval = np.load(os.path.join(data_folder,\"forecast_data\",\"times_trainval.npy\"),allow_pickle=True)\n",
    "print(\"times_trainval.shape:\", times_trainval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e51f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(times_trainval)\n",
    "batch_size = num_samples//5\n",
    "indices = np.arange(num_samples)\n",
    "print('-'*50)\n",
    "print('data reading start...')\n",
    "for i in range(num_samples // batch_size):\n",
    "    start_time = time.time()\n",
    "    start_idx = (i * batch_size) % num_samples\n",
    "    if i<num_samples // batch_size-1:\n",
    "        idxs = indices[start_idx:start_idx + batch_size]\n",
    "    else:\n",
    "        idxs = indices[start_idx:]\n",
    "    _ = forecast_dataset['trainval']['images_log'][idxs]\n",
    "    _ = forecast_dataset['trainval']['si_pred'][idxs]\n",
    "    _ = forecast_dataset['trainval']['cloud_fraction'][idxs]\n",
    "    end_time = time.time()\n",
    "    print(\"batch {0} samples: {1} to {2}, {3:.2f}% finished, processing time {4:.2f}s\"\n",
    "          .format(i+1, idxs[0],idxs[-1],(idxs[-1]/num_samples)*100,(end_time-start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2d7263",
   "metadata": {},
   "source": [
    "### Input data pipeline helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8323da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_block_shuffle(times_trainval):\n",
    "    \n",
    "    dates_trainval = np.zeros_like(times_trainval, dtype=datetime.date)\n",
    "    for i in range(len(times_trainval)):\n",
    "        dates_trainval[i] = times_trainval[i].date()\n",
    "\n",
    "    unique_dates = np.unique(dates_trainval)\n",
    "    blocks = []\n",
    "    for i in range(len(unique_dates)):\n",
    "        blocks.append(np.where(dates_trainval == unique_dates[i])[0])\n",
    "\n",
    "    np.random.seed(1)\n",
    "    np.random.shuffle(blocks)\n",
    "    shuffled_indices = np.asarray(list(itertools.chain.from_iterable(blocks)))\n",
    "\n",
    "    return shuffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_split(split_data, fold_index, num_fold):\n",
    "    '''\n",
    "    input:\n",
    "    split_data: the dayblock shuffled indices to be splitted\n",
    "    fold_index: the ith fold chosen as the validation, used for generating the seed for random shuffling\n",
    "    num_fold: N-fold cross validation\n",
    "    output:\n",
    "    data_train: the train data indices\n",
    "    data_val: the validation data indices\n",
    "    '''\n",
    "    num_samples = len(split_data)\n",
    "    indices = np.arange(num_samples)\n",
    "\n",
    "    val_mask = np.zeros(len(indices), dtype=bool)\n",
    "    val_mask[int(fold_index / num_fold * num_samples):int((fold_index + 1) / num_fold * num_samples)] = True\n",
    "    val_indices = indices[val_mask]\n",
    "    train_indices = indices[np.logical_not(val_mask)]\n",
    "\n",
    "    np.random.seed(fold_index)\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(val_indices)\n",
    "    \n",
    "    data_train = split_data[train_indices]\n",
    "    data_val = split_data[val_indices]\n",
    "\n",
    "    return data_train,data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_data):\n",
    "    '''\n",
    "    image data processing: reshaping and normalization\n",
    "    '''\n",
    "    image_data = tf.transpose(image_data,perm=[0,2,3,1,4])\n",
    "    image_data = tf.reshape(image_data, [image_data.shape[0],image_data.shape[1],image_data.shape[2],-1])\n",
    "\n",
    "    image_data = tf.image.convert_image_dtype(image_data, tf.float32)\n",
    "\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d738abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(hdf5_data_path,sample_idx,batch_size=256):\n",
    "    '''\n",
    "    input:\n",
    "    hdf5_data_path: path to hdf5 data file\n",
    "    sample_idx: \n",
    "        for training and validation:\n",
    "            dayblock shuffled indices with cross-validation split into training and validation\n",
    "            either training or validation indices will be input\n",
    "        for testing: the indices are not shuffled\n",
    "    is_trainval: a flag, True for trainig and validation\n",
    "    output:\n",
    "    dataset: dataset for training, validation\n",
    "    '''\n",
    "\n",
    "    def mapping_func_py(hdf5_data_path,sample_idx):\n",
    "        '''\n",
    "        mapping indices to corresponding images and irraidance data in hdf5 (python expression)\n",
    "        '''\n",
    "        hdf5_data_path = hdf5_data_path.numpy().decode() \n",
    "        sample_idx = sorted(sample_idx.numpy())\n",
    "\n",
    "        with h5py.File(hdf5_data_path,'r') as f:\n",
    "\n",
    "            images_log = f['trainval']['images_log'][sample_idx]\n",
    "            si_pred = f['trainval']['si_pred'][sample_idx]\n",
    "            cloud_fraction = f['trainval']['cloud_fraction'][sample_idx]\n",
    "        \n",
    "        \n",
    "            images_log = process_image(images_log)\n",
    "                     \n",
    "            si_pred = tf.convert_to_tensor(si_pred, dtype=tf.float32) \n",
    "            cloud_fraction = tf.convert_to_tensor(cloud_fraction, dtype=tf.float32) \n",
    "        \n",
    "            return images_log,cloud_fraction, si_pred #GHI_rad, si_pred  #\n",
    "\n",
    "    def mapping_func_tf(hdf5_data_path,sample_idx):\n",
    "        '''\n",
    "        a wrapper mapping function to get the nested data structure \n",
    "        the output type of tf.py_function cannot be a nested sequence when using a tf.py_function with the tf.data API\n",
    "        '''\n",
    "        images_log,cloud_fraction, si_pred = tf.py_function(func=mapping_func_py,\n",
    "                                                           inp=[hdf5_data_path, sample_idx], \n",
    "                                                           Tout=(tf.float32, tf.float32,tf.float32))\n",
    "        return (images_log, cloud_fraction), si_pred\n",
    "    \n",
    "    \n",
    "    idx_ds = tf.data.Dataset.from_tensor_slices(sample_idx)\n",
    "    \n",
    "    idx_ds = idx_ds.shuffle(buffer_size = idx_ds.cardinality().numpy(),seed=0)\n",
    "    idx_ds = idx_ds.batch(batch_size).repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    dataset = idx_ds.map(lambda x: mapping_func_tf(hdf5_data_path,x),\n",
    "                         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0680ab",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cbd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters = 24\n",
    "kernel_size = [3,3]\n",
    "pool_size = [2,2]\n",
    "strides = 2\n",
    "dense_size = 1024\n",
    "drop_rate = 0.4\n",
    "\n",
    "num_epochs = 200 #(The maximum epoches set to 200 and there might be early stopping depends on validation loss)\n",
    "num_fold = 10 # 10-fold cross-validation\n",
    "batch_size = 256\n",
    "learning_rate = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45882ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_lstm_model():\n",
    "    x_in = keras.Input(shape=image_input_dim)\n",
    "    \n",
    "    x2_in = keras.Input(shape=num_log_term)\n",
    "\n",
    "    x = keras.layers.Conv2D(num_filters,kernel_size,padding=\"same\",activation='relu')(x_in)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size, strides)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(num_filters*2,kernel_size,padding=\"same\",activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size, strides)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    \n",
    "    x = keras.layers.Concatenate(axis=1)([x, x2_in])\n",
    "    \n",
    "    x = keras.layers.Reshape((1, -1))(x) \n",
    "    \n",
    "    lstm_model =keras.layers.RNN(keras.layers.LSTMCell(units=32))(x)\n",
    "    cnn_lstm = keras.layers.Dense(dense_size, activation='relu')(lstm_model)\n",
    "    cnn_lstm = keras.layers.Dropout(drop_rate)(cnn_lstm)\n",
    "    cnn_lstm = keras.layers.Dense(dense_size, activation='relu')(cnn_lstm)\n",
    "    cnn_lstm = keras.layers.Dropout(drop_rate)(cnn_lstm)\n",
    "\n",
    "    y_out = keras.layers.Dense(units=1)(cnn_lstm)\n",
    "\n",
    "    model = keras.Model(inputs=[x_in, x2_in],outputs=y_out)\n",
    "\n",
    "    return model\n",
    "cnn_lstm_model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d184e30",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc1c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_dayblock_shuffled = day_block_shuffle(times_trainval)\n",
    "\n",
    "train_loss_hist = []\n",
    "val_loss_hist = []\n",
    "    \n",
    "for i in range(num_fold):\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "    model = cnn_lstm_model()\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate),loss='mse')\n",
    "    \n",
    "    print('Repetition {0} model training started ...'.format(i+1))\n",
    "    \n",
    "    save_directory = os.path.join(output_folder,'repetition_'+str(i+1))\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "        \n",
    "    indices_train, indices_val = cv_split(indices_dayblock_shuffled,i,num_fold)\n",
    "    \n",
    "    ds_train_batched = data_loader(data_path,indices_train)#, batch_size=batch_size)\n",
    "    ds_val_batched = data_loader(data_path,indices_val,batch_size=600)\n",
    "\n",
    "    earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(os.path.join(save_directory,'best_model_repitition_'+str(i+1)+'.h5'), \n",
    "                                monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "    history = model.fit(ds_train_batched, epochs=num_epochs, steps_per_epoch=len(indices_train)//batch_size+1,\n",
    "                                   verbose=1, callbacks=[earlystop,checkpoint], validation_data=ds_val_batched,\n",
    "                                  validation_steps=len(indices_val)//batch_size+1)\n",
    "    train_loss_hist.append(history.history['loss'])\n",
    "    val_loss_hist.append(history.history['val_loss'])\n",
    "\n",
    "    plt.plot(train_loss_hist[i],label='train')\n",
    "    plt.plot(val_loss_hist[i],label='validation')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    np.save(os.path.join(output_folder,'train_loss_hist.npy'),train_loss_hist)\n",
    "    np.save(os.path.join(output_folder,'val_loss_hist.npy'),val_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_train_loss_MSE = np.zeros(num_fold)\n",
    "best_val_loss_MSE = np.zeros(num_fold)\n",
    "\n",
    "for i in range(num_fold):\n",
    "    best_val_loss_MSE[i] = np.min(val_loss_hist[i])\n",
    "    idx = np.argmin(val_loss_hist[i])\n",
    "    best_train_loss_MSE[i] = train_loss_hist[i][idx]\n",
    "    print('Model {0}  -- train loss: {1:.2f}, validation loss: {2:.2f} (RMSE)'.format(i+1, np.sqrt(best_train_loss_MSE[i]), np.sqrt(best_val_loss_MSE[i])))\n",
    "print('The mean train loss (RMSE) for all models is {0:.2f}'.format(np.mean(np.sqrt(best_train_loss_MSE))))\n",
    "print('The mean validation loss (RMSE) for all models is {0:.2f}'.format(np.mean(np.sqrt(best_val_loss_MSE))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d7c65",
   "metadata": {},
   "source": [
    "## 3. Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154da847",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_test = np.load(os.path.join(data_folder,'forecast_data',\"times_test.npy\"),allow_pickle=True)\n",
    "print(\"times_test.shape:\", times_test.shape)\n",
    "\n",
    "with h5py.File(data_path,'r') as f:\n",
    "\n",
    "    images_log_test = f['test']['images_log'][...]\n",
    "    si_pred_test = f['test']['si_pred'][...]\n",
    "    si_log_test = f['test']['si_log'][...]\n",
    "    cloud_fraction_test = f['test']['cloud_fraction'][...]\n",
    "    \n",
    "    \n",
    "images_log_test = np.transpose(images_log_test,axes=[0,2,3,1,4])\n",
    "images_log_test = np.reshape(images_log_test, [images_log_test.shape[0],images_log_test.shape[1],images_log_test.shape[2],-1])\n",
    "images_log_test = (images_log_test/255.0).astype('float32')\n",
    "\n",
    "\n",
    "    \n",
    "print(\"images_log_test.shape:\",images_log_test.shape)\n",
    "print(\"si_pred_test.shape:\",si_pred_test.shape)\n",
    "print(\"si_log_test.shape:\",si_log_test.shape)\n",
    "print(\"cloud_fraction_test.shape:\",cloud_fraction_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f047976",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.zeros((num_fold,len(times_test)))\n",
    "prediction = np.zeros((num_fold,len(times_test)))\n",
    "\n",
    "with tf.device ('CPU'):\n",
    "    \n",
    "    for i in range(num_fold):\n",
    "        print(\"loading repetition {0} model ...\".format(i+1))\n",
    "        model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
    "        model = keras.models.load_model(model_path)#,  compile=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"evaluating performance for the model\".format(i+1))\n",
    "        loss[i] = model.evaluate(x=[images_log_test,cloud_fraction_test], y=si_pred_test, batch_size=200, verbose=1)\n",
    "        \n",
    "        print(\"generating predictions for the model\".format(i+1))\n",
    "        prediction[i] = np.squeeze(model.predict([images_log_test,cloud_fraction_test], batch_size=200, verbose=1))\n",
    "        prediction[i] =  prediction[i]*1000\n",
    "        \n",
    "si_pred_test = si_pred_test*1000\n",
    "\n",
    "np.save(os.path.join(output_folder,'test_predictions.npy'),prediction)\n",
    "\n",
    "print('-'*50)\n",
    "print(\"model ensembling ...\")\n",
    "prediction_ensemble = np.mean(prediction,axis=0)\n",
    "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-si_pred_test)**2))\n",
    "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sunny_dates =[(2024,7,29)]\n",
    "partially_cloudy_dates = [(2024,7,28)]\n",
    "cloudy_dates =[(2024,7,27)]\n",
    "\n",
    "\n",
    "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
    "partially_cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in partially_cloudy_dates]\n",
    "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
    "\n",
    "dates_test = np.asarray([times.date() for times in times_test])\n",
    "\n",
    "mask_sunny_dates = np.zeros(len(si_pred_test),dtype=bool)\n",
    "for i in sunny_dates_test:\n",
    "    mask_sunny_dates[np.where(dates_test==i)[0]]=1\n",
    "\n",
    "mask_partially_cloudy_dates = np.zeros(len(si_pred_test),dtype=bool)\n",
    "for i in partially_cloudy_dates_test:\n",
    "    mask_partially_cloudy_dates[np.where(dates_test==i)[0]]=1\n",
    "\n",
    "mask_cloudy_dates =np.zeros(len(si_pred_test),dtype=bool)\n",
    "for i in cloudy_dates_test:\n",
    "    mask_cloudy_dates[np.where(dates_test==i)[0]]=1\n",
    "\n",
    "    \n",
    "times_test_sunny = times_test[mask_sunny_dates]\n",
    "si_pred_test_sunny = si_pred_test[mask_sunny_dates]\n",
    "si_log_test_sunny = si_log_test[mask_sunny_dates]\n",
    "prediction_ensemble_sunny = prediction_ensemble[mask_sunny_dates]\n",
    "print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n",
    "\n",
    "\n",
    "times_test_partially_cloudy = times_test[mask_partially_cloudy_dates]\n",
    "si_pred_test_partially_cloudy = si_pred_test[mask_partially_cloudy_dates]\n",
    "si_log_test_partially_cloudy = si_log_test[mask_partially_cloudy_dates]\n",
    "prediction_ensemble_partially_cloudy = prediction_ensemble[mask_partially_cloudy_dates]\n",
    "print(\"times_test_partially_cloudy.shape:\",times_test_partially_cloudy.shape)\n",
    "\n",
    "times_test_cloudy = times_test[mask_cloudy_dates]\n",
    "si_pred_test_cloudy= si_pred_test[mask_cloudy_dates]\n",
    "si_log_test_cloudy = si_log_test[mask_cloudy_dates]\n",
    "prediction_ensemble_cloudy = prediction_ensemble[mask_cloudy_dates]\n",
    "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5b67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(si_pred_test)\n",
    "\n",
    "df1=df[:863].squeeze()\n",
    "df1 =df1.shift(15)\n",
    "df1=df1.replace([np.nan, -np.inf], 0)\n",
    "\n",
    "df2=df[863:865+861]#.squeeze()\n",
    "df2 =df2.shift(15)\n",
    "df2=df2.replace([np.nan, -np.inf], 0)\n",
    "\n",
    "\n",
    "df3=df[865+861:]#.squeeze()\n",
    "df3 =df3.shift(15)\n",
    "df3=df3.replace([np.nan, -np.inf], 0)\n",
    "\n",
    "cnn_lstm_prediction =pd.DataFrame(prediction_ensemble.squeeze())\n",
    "testing_timeas=pd.DataFrame(times_test.squeeze())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('pesist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d402fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('pesist.csv') \n",
    "cnn_lstm_prediction.to_csv('15min_cnn_lstm.csv') \n",
    "testing_timeas.to_csv(\"test_times.csv\")\n",
    "df1.to_csv('clear_persist.csv')\n",
    "df2.to_csv('partly_persist.csv')\n",
    "df3.to_csv('cloudy_persist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae7ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence_prediction = pd.concat([df1,df2,df3]).squeeze().to_numpy() \n",
    "\n",
    "sunny_persistence_predictions = df1.to_numpy()\n",
    "\n",
    "partly_cloudy_persistence_predictions = (df2.to_numpy())\n",
    "\n",
    "cloudy_persistence_predictions = df3.to_numpy()\n",
    "\n",
    "\n",
    "rmse_sunny_persistence = np.sqrt(np.mean(np.square((sunny_persistence_predictions-si_pred_test_sunny))))\n",
    "rmse_partly_cloudy_persistence = np.sqrt(np.mean(np.square((partly_cloudy_persistence_predictions-si_pred_test_partially_cloudy))))\n",
    "rmse_cloudy_persistence = np.sqrt(np.mean(np.square((cloudy_persistence_predictions-si_pred_test_cloudy))))\n",
    "rmse_persistence_overall = np.sqrt((rmse_sunny_persistence**2*len(si_pred_test_sunny)+rmse_cloudy_persistence **2*len(si_pred_test_cloudy)+rmse_partly_cloudy_persistence**2*len(si_pred_test_partially_cloudy))/(len(si_pred_test)))\n",
    "\n",
    "print(\"sunny days persistence RMSE: {0:.3f}\".format(rmse_sunny_persistence))\n",
    "print(\"partly cloudy days persistence RMSE: {0:.3f}\".format(rmse_partly_cloudy_persistence))\n",
    "print(\"cloudy days persistence RMSE: {0:.3f}\".format(rmse_cloudy_persistence))\n",
    "print(\"persistence overall RSME: {0:.2f}\".format(rmse_persistence_overall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd024241",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_persistnce_sunny = np.mean(np.abs((sunny_persistence_predictions-si_pred_test_sunny)))\n",
    "mae_persistnece_partially_cloudy = np.mean(np.abs((partly_cloudy_persistence_predictions-si_pred_test_partially_cloudy)))\n",
    "mae_persistence_cloudy = np.mean(np.abs((cloudy_persistence_predictions-si_pred_test_cloudy)))\n",
    "mae_persistence_overall = (mae_persistence_cloudy*len(si_pred_test_cloudy) + mae_persistnece_partially_cloudy*len(si_pred_test_partially_cloudy)+mae_persistnce_sunny*len(si_pred_test_sunny))/(len(si_pred_test))\n",
    "\n",
    "print(\"persistence sunny days MAE: {0:.2f}\".format(mae_persistnce_sunny))\n",
    "print(\"persistence partially cloudy days MAE: {0:.2f}\".format(mae_persistnece_partially_cloudy))\n",
    "print(\"persistence cloudy days MAE: {0:.2f}\".format(mae_persistence_cloudy))\n",
    "print(\"persistennce overall MAE: {0:.2f}\".format(mae_persistence_overall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdba91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_sunny = np.sqrt(np.mean(np.square((prediction_ensemble_sunny-si_pred_test_sunny))))\n",
    "rmse_partially_cloudy=np.sqrt(np.mean(np.square((prediction_ensemble_partially_cloudy-si_pred_test_partially_cloudy))))\n",
    "rmse_cloudy = np.sqrt(np.mean(np.square((prediction_ensemble_cloudy-si_pred_test_cloudy))))\n",
    "rmse_overall = np.sqrt((rmse_sunny**2*len(si_pred_test_sunny)+rmse_cloudy**2*len(si_pred_test_cloudy)+rmse_partially_cloudy**2*len(si_pred_test_partially_cloudy))/(len(si_pred_test)))\n",
    "\n",
    "print(\"test set sunny days RMSE: {0:.3f}\".format(rmse_sunny))\n",
    "print(\"test set partially cloudy days RMSE: {0:.3f}\".format(rmse_partially_cloudy))\n",
    "print(\"test set cloudy days RMSE: {0:.3f}\".format(rmse_cloudy))\n",
    "print(\"test set overall RMSE: {0:.3f}\".format(rmse_overall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2543b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_sunny = np.mean(np.abs((prediction_ensemble_sunny-si_pred_test_sunny)))\n",
    "mae_partially_cloudy = np.mean(np.abs((prediction_ensemble_partially_cloudy-si_pred_test_partially_cloudy)))\n",
    "mae_cloudy = np.mean(np.abs((prediction_ensemble_cloudy-si_pred_test_cloudy)))\n",
    "mae_overall = (mae_cloudy*len(si_pred_test_cloudy) + mae_partially_cloudy*len(si_pred_test_partially_cloudy)+mae_sunny*len(si_pred_test_sunny))/(len(si_pred_test))\n",
    "\n",
    "print(\"test set sunny days MAE: {0:.2f}\".format(mae_sunny))\n",
    "print(\"test set partially cloudy days MAE: {0:.2f}\".format(mae_partially_cloudy))\n",
    "print(\"test set cloudy days MAE: {0:.2f}\".format(mae_cloudy))\n",
    "print(\"test set overall MAE: {0:.2f}\".format(mae_overall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
    "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
    "\n",
    "f,axarr = plt.subplots(2,3,sharex=True, sharey = True)\n",
    "xfmt = mdates.DateFormatter('%H:%M')\n",
    "fmt_date = datetime.date(2000,1,1)\n",
    "\n",
    "green = '#8AB8A7'\n",
    "red = '#B83A4B'\n",
    "blue = '#67AFD2'\n",
    "grey =  '#B6B1A9'\n",
    "\n",
    "red1= \"#FA3C3C\"\n",
    "\n",
    "\n",
    "for i,date in enumerate(sunny_dates_test):\n",
    "    ax = axarr[i,0]\n",
    "    date_mask = (dates_test == date)\n",
    "    \n",
    "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]] \n",
    "  \n",
    "    rmse = np.sqrt(np.mean(np.square((si_pred_test[date_mask]-prediction_ensemble[date_mask]))))\n",
    "    rmse_persistence=np.sqrt(np.mean(np.square((si_pred_test[date_mask]-persistence_prediction[date_mask])))) \n",
    "\n",
    "    mae = np.mean(np.abs((si_pred_test[date_mask]-prediction_ensemble[date_mask])))\n",
    "    mae_persistence = np.mean(np.abs((si_pred_test[date_mask]-persistence_prediction[date_mask])))\n",
    "    \n",
    "    \n",
    "    ax.plot(hours_xaxis, si_pred_test[date_mask], linewidth = 2,color= \"k\", label = 'Measured')\n",
    "   \n",
    "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 2,label = 'CNN-LSTM',color= 'tab:red',markerfacecolor=\"None\")\n",
    "    ax.plot(hours_xaxis,persistence_prediction[date_mask] ,linewidth = 2,label = ' Persistence',color=\"tab:blue\",markerfacecolor=\"None\") \n",
    "    \n",
    "    ax.set_ylabel('GHI (W/m2)')\n",
    "    ax.xaxis.set_major_formatter(xfmt)\n",
    "    ax.text(0.05,0.88,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes,color='tab:red', fontsize=\"12\")\n",
    "\n",
    "\n",
    "for i,date in enumerate(partially_cloudy_dates_test):\n",
    "    ax = axarr[i,1]\n",
    "    date_mask = (dates_test == date)\n",
    "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]] \n",
    "    \n",
    "    rmse = np.sqrt(np.mean(np.square((si_pred_test[date_mask]-prediction_ensemble[date_mask]))))\n",
    "\n",
    "    mae = np.mean(np.abs((si_pred_test[date_mask]-prediction_ensemble[date_mask])))\n",
    "    \n",
    "    ax.plot(hours_xaxis, si_pred_test[date_mask], linewidth = 2,color='k')\n",
    "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth =2,label = 'CNN-LSTM',color=\"tab:red\",markerfacecolor=\"None\")\n",
    "\n",
    "    ax.xaxis.set_major_formatter(xfmt)\n",
    "    ax.text(0.05,0.88,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes,color='tab:red', fontsize=\"12\")\n",
    "    \n",
    "    \n",
    "for i,date in enumerate(cloudy_dates_test):\n",
    "    ax = axarr[i,2]\n",
    "    date_mask = (dates_test == date)\n",
    "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]] \n",
    "    \n",
    "    rmse = np.sqrt(np.mean(np.square((si_pred_test[date_mask]-prediction_ensemble[date_mask]))))\n",
    "\n",
    "    mae = np.mean(np.abs((si_pred_test[date_mask]-prediction_ensemble[date_mask])))\n",
    "    \n",
    "    ax.plot(hours_xaxis, si_pred_test[date_mask], linewidth =2,color='k')\n",
    "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth =2,label = 'CNN-LSTM',color='tab:red',markerfacecolor=\"None\")\n",
    "\n",
    "\n",
    "    \n",
    "    ax.xaxis.set_major_formatter(xfmt)\n",
    "    ax.text(0.05,0.88,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes,color='tab:red', fontsize=\"12\")\n",
    "    \n",
    "axarr[0,0].set_ylim(0, 1200)\n",
    "axarr[0,0].legend(bbox_to_anchor= [1.6,1.3], loc = 'upper center', ncol = 3)\n",
    "axarr[-1,0].set_xlabel('Time')\n",
    "axarr[-1,1].set_xlabel('Time')\n",
    "axarr[-1,-1].set_xlabel('Time')\n",
    "\n",
    "axarr[0,0].legend(loc='upper right', fontsize=\"12\") \n",
    "\n",
    "f.set_size_inches(18,12)\n",
    "f.tight_layout(pad=1.5) \n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
